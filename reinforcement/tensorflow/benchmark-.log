STARTING TIMING RUN AT 2018-06-11 09:49:07 PM
running benchmark with seed 1
Creating random initial weights...
Bootstrapping with working dir /research/results/minigo/final/estimator_working_dir
 Model 0 exported to /research/results/minigo/final/models/000000-bootstrap
working_dir /research/results/minigo/final/estimator_working_dir
model_path /research/results/minigo/final/models/000000-bootstrap
Starting self play loop.
Starting Worker...
Self play worker: setting random seed =  3
Starting Worker...
Self play worker: setting random seed =  4
Starting Worker...
Self play worker: setting random seed =  5
Starting Worker...
Self play worker: setting random seed =  6
Starting Worker...
Self play worker: setting random seed =  7
Starting Worker...
Self play worker: setting random seed =  8
Starting Worker...
Playing a game with model 000000-bootstrap
[array([[[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       ...,

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]]], dtype=uint8)]
Traceback (most recent call last):
  File "selfplay_worker.py", line 276, in <module>
    rl_loop()
  File "selfplay_worker.py", line 251, in rl_loop
    selfplay_cache_model(network, model_name)
  File "selfplay_worker.py", line 159, in selfplay_cache_model
    verbose=verbose,
  File "/research/reinforcement/minigo/main.py", line 263, in selfplay_cache_model
    network, readouts, resign_threshold, verbose)
  File "/research/reinforcement/minigo/selfplay_mcts.py", line 49, in play
    prob, val = network.run(first_node.position)
  File "/research/reinforcement/minigo/dual_net.py", line 40, in run
    use_random_symmetry=use_random_symmetry)
  File "/research/reinforcement/minigo/dual_net.py", line 50, in run_many
    probabilities, value = self.model(processed)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/research/reinforcement/minigo/dual_net.py", line 148, in forward
    initial_output = self.initial_layer(features)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
TypeError: conv2d(): argument 'input' (position 1) must be Tensor, not list
Playing a game with model 000000-bootstrap
[array([[[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       ...,

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]]], dtype=uint8)]
Traceback (most recent call last):
  File "selfplay_worker.py", line 276, in <module>
    rl_loop()
  File "selfplay_worker.py", line 251, in rl_loop
    selfplay_cache_model(network, model_name)
  File "selfplay_worker.py", line 159, in selfplay_cache_model
    verbose=verbose,
  File "/research/reinforcement/minigo/main.py", line 263, in selfplay_cache_model
    network, readouts, resign_threshold, verbose)
  File "/research/reinforcement/minigo/selfplay_mcts.py", line 49, in play
    prob, val = network.run(first_node.position)
  File "/research/reinforcement/minigo/dual_net.py", line 40, in run
    use_random_symmetry=use_random_symmetry)
  File "/research/reinforcement/minigo/dual_net.py", line 50, in run_many
    probabilities, value = self.model(processed)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/research/reinforcement/minigo/dual_net.py", line 148, in forward
    initial_output = self.initial_layer(features)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
TypeError: conv2d(): argument 'input' (position 1) must be Tensor, not list
Self play worker: setting random seed =  9
Starting Worker...
Playing a game with model 000000-bootstrap
[array([[[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       ...,

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]]], dtype=uint8)]
Traceback (most recent call last):
  File "selfplay_worker.py", line 276, in <module>
    rl_loop()
  File "selfplay_worker.py", line 251, in rl_loop
    selfplay_cache_model(network, model_name)
  File "selfplay_worker.py", line 159, in selfplay_cache_model
    verbose=verbose,
  File "/research/reinforcement/minigo/main.py", line 263, in selfplay_cache_model
    network, readouts, resign_threshold, verbose)
  File "/research/reinforcement/minigo/selfplay_mcts.py", line 49, in play
    prob, val = network.run(first_node.position)
  File "/research/reinforcement/minigo/dual_net.py", line 40, in run
    use_random_symmetry=use_random_symmetry)
  File "/research/reinforcement/minigo/dual_net.py", line 50, in run_many
    probabilities, value = self.model(processed)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/research/reinforcement/minigo/dual_net.py", line 148, in forward
    initial_output = self.initial_layer(features)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
TypeError: conv2d(): argument 'input' (position 1) must be Tensor, not list
Starting Worker...
Self play worker: setting random seed =  10
Playing a game with model 000000-bootstrap
[array([[[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       ...,

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]]], dtype=uint8)]
Traceback (most recent call last):
  File "selfplay_worker.py", line 276, in <module>
    rl_loop()
  File "selfplay_worker.py", line 251, in rl_loop
    selfplay_cache_model(network, model_name)
  File "selfplay_worker.py", line 159, in selfplay_cache_model
    verbose=verbose,
  File "/research/reinforcement/minigo/main.py", line 263, in selfplay_cache_model
    network, readouts, resign_threshold, verbose)
  File "/research/reinforcement/minigo/selfplay_mcts.py", line 49, in play
    prob, val = network.run(first_node.position)
  File "/research/reinforcement/minigo/dual_net.py", line 40, in run
    use_random_symmetry=use_random_symmetry)
  File "/research/reinforcement/minigo/dual_net.py", line 50, in run_many
    probabilities, value = self.model(processed)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/research/reinforcement/minigo/dual_net.py", line 148, in forward
    initial_output = self.initial_layer(features)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
TypeError: conv2d(): argument 'input' (position 1) must be Tensor, not list
Self play worker: setting random seed =  11
Starting Worker...
Playing a game with model 000000-bootstrap
[array([[[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       ...,

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]]], dtype=uint8)]
Traceback (most recent call last):
  File "selfplay_worker.py", line 276, in <module>
    rl_loop()
  File "selfplay_worker.py", line 251, in rl_loop
    selfplay_cache_model(network, model_name)
  File "selfplay_worker.py", line 159, in selfplay_cache_model
    verbose=verbose,
  File "/research/reinforcement/minigo/main.py", line 263, in selfplay_cache_model
    network, readouts, resign_threshold, verbose)
  File "/research/reinforcement/minigo/selfplay_mcts.py", line 49, in play
    prob, val = network.run(first_node.position)
  File "/research/reinforcement/minigo/dual_net.py", line 40, in run
    use_random_symmetry=use_random_symmetry)
  File "/research/reinforcement/minigo/dual_net.py", line 50, in run_many
    probabilities, value = self.model(processed)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/research/reinforcement/minigo/dual_net.py", line 148, in forward
    initial_output = self.initial_layer(features)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
TypeError: conv2d(): argument 'input' (position 1) must be Tensor, not list
Self play worker: setting random seed =  12
Starting Worker...
^XPlaying a game with model 000000-bootstrap
[array([[[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       ...,

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 1]]], dtype=uint8)]
Traceback (most recent call last):
  File "selfplay_worker.py", line 276, in <module>
    rl_loop()
  File "selfplay_worker.py", line 251, in rl_loop
    selfplay_cache_model(network, model_name)
  File "selfplay_worker.py", line 159, in selfplay_cache_model
    verbose=verbose,
  File "/research/reinforcement/minigo/main.py", line 263, in selfplay_cache_model
    network, readouts, resign_threshold, verbose)
  File "/research/reinforcement/minigo/selfplay_mcts.py", line 49, in play
    prob, val = network.run(first_node.position)
  File "/research/reinforcement/minigo/dual_net.py", line 40, in run
    use_random_symmetry=use_random_symmetry)
  File "/research/reinforcement/minigo/dual_net.py", line 50, in run_many
    probabilities, value = self.model(processed)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/research/reinforcement/minigo/dual_net.py", line 148, in forward
    initial_output = self.initial_layer(features)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
TypeError: conv2d(): argument 'input' (position 1) must be Tensor, not list
Self play worker: setting random seed =  13
Starting Worker...
^CTraceback (most recent call last):
  File "loop_selfplay.py", line 211, in <module>
Traceback (most recent call last):
  File "selfplay_worker.py", line 22, in <module>
    import main
  File "/research/reinforcement/minigo/main.py", line 29, in <module>
    import torch
  File "/usr/local/lib/python3.5/dist-packages/torch/__init__.py", line 277, in <module>
    main_()
  File "loop_selfplay.py", line 145, in main_
    time.sleep(1)
    import torch.optim
  File "/usr/local/lib/python3.5/dist-packages/torch/optim/__init__.py", line 14, in <module>
KeyboardInterrupt
    from .sgd import SGD
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 954, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 896, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1139, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1113, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1221, in find_spec
  File "<frozen importlib._bootstrap_external>", line 75, in _path_stat
KeyboardInterrupt
Traceback (most recent call last):
  File "selfplay_worker.py", line 276, in <module>
    rl_loop()
  File "selfplay_worker.py", line 242, in rl_loop
    network = selfplay_laod_model(model_name)
  File "selfplay_worker.py", line 220, in selfplay_laod_model
    network = dual_net.DualNetwork(load_file)
  File "/research/reinforcement/minigo/dual_net.py", line 29, in __init__
    self.initialize_graph()
  File "/research/reinforcement/minigo/dual_net.py", line 34, in initialize_graph
    self.model.cuda()
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 249, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 176, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 176, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 182, in _apply
    param.data = fn(param.data)
  File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 249, in <lambda>
    return self._apply(lambda t: t.cuda(device))
  File "/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py", line 146, in _lazy_init
    def _lazy_init():
KeyboardInterrupt
